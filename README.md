# Markovian-Transformer
Markovian Transformers blend the stepwise transition modeling of Markov chains with the global context sensitivity of Transformers. A local Markovian attention block captures sequential dependencies via learned transition probabilities, while a global head ensures long-range understandingâ€”enabling robust modeling of complex sequences.
